%! TEX program = xelatex
\documentclass[acmsmall,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}

%%% Typesetting for listings
\usepackage{listings}
\setmonofont{JuliaMono}[Scale=MatchLowercase]

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{ltblue}{rgb}{0,0.4,0.4}
\definecolor{dkviolet}{rgb}{0.3,0,0.5}

%%% lstlisting coq style (inspired from a file of Assia Mahboubi)
\lstdefinelanguage{Coq}{ 
    % Anything betweeen $ becomes LaTeX math mode
    mathescape=true,
    % Comments may or not include Latex commands
    texcl=false, 
    % Vernacular commands
    morekeywords=[1]{Section, Module, End, Require, Import, Export,
        Variable, Variables, Parameter, Parameters, Axiom, Hypothesis,
        Hypotheses, Notation, Local, Tactic, Reserved, Scope, Open, Close,
        Bind, Delimit, Definition, Let, Ltac, Fixpoint, CoFixpoint, Add,
        Morphism, Relation, Implicit, Arguments, Unset, Contextual,
        Strict, Prenex, Implicits, Inductive, CoInductive, Record,
        Structure, Canonical, Coercion, Context, Class, Global, Instance,
        Program, Infix, Theorem, Lemma, Corollary, Proposition, Fact,
        Remark, Example, Proof, Goal, Save, Qed, Defined, Hint, Resolve,
        Rewrite, View, Search, Show, Print, Printing, All, Eval, Check,
        Projections, inside, outside, Def},
    % Gallina
    morekeywords=[2]{forall, exists, exists2, fun, fix, cofix, struct,
        match, with, end, as, in, return, let, if, is, then, else, for, of,
        nosimpl, when},
    % Sorts
    morekeywords=[3]{Type, Prop, Set, true, false, option},
    % Various tactics, some are std Coq subsumed by ssr, for the manual purpose
    morekeywords=[4]{pose, set, move, case, elim, apply, clear, hnf,
        intro, intros, generalize, rename, pattern, after, destruct,
        induction, using, refine, inversion, injection, rewrite, congr,
        unlock, compute, ring, field, fourier, replace, fold, unfold,
        change, cutrewrite, simpl, have, suff, wlog, suffices, without,
        loss, nat_norm, assert, cut, trivial, revert, bool_congr, nat_congr,
        symmetry, transitivity, auto, split, left, right, autorewrite},
    % Terminators
    morekeywords=[5]{by, done, exact, reflexivity, tauto, romega, omega,
        assumption, solve, contradiction, discriminate},
    % Control
    morekeywords=[6]{do, last, first, try, idtac, repeat},
    % Comments delimiters, we do turn this off for the manual
    morecomment=[s]{(*}{*)},
    % Spaces are not displayed as a special character
    showstringspaces=false,
    % String delimiters
    morestring=[b]",
    morestring=[d],
    % Size of tabulations
    tabsize=3,
    % Enables ASCII chars 128 to 255
    extendedchars=false,
    % Case sensitivity
    sensitive=true,
    % Automatic breaking of long lines
    breaklines=false,
    % Default style fors listings
    basicstyle=\small,
    % Position of captions is bottom
    captionpos=b,
    % flexible columns
    columns=[l]flexible,
    % Style for (listings') identifiers
    identifierstyle={\ttfamily\color{black}},
    % Style for declaration keywords
    keywordstyle=[1]{\ttfamily\color{dkviolet}},
    % Style for gallina keywords
    keywordstyle=[2]{\ttfamily\color{dkgreen}},
    % Style for sorts keywords
    keywordstyle=[3]{\ttfamily\color{ltblue}},
    % Style for tactics keywords
    keywordstyle=[4]{\ttfamily\color{dkblue}},
    % Style for terminators keywords
    keywordstyle=[5]{\ttfamily\color{dkred}},
    %Style for iterators
    %keywordstyle=[6]{\ttfamily\color{dkpink}},
    % Style for strings
    stringstyle=\ttfamily,
    % Style for comments
    commentstyle={\ttfamily\color{dkgreen}},
    %moredelim=**[is][\ttfamily\color{red}]{/&}{&/},
    literate=
    {\\forall}{{\color{dkgreen}{$\forall\;$}}}1
    {\\exists}{{$\exists\;$}}1
    {<-}{{$\leftarrow\;$}}1
    {=>}{{$\Rightarrow\;$}}1
    {==}{{\code{==}\;}}1
    {==>}{{\code{==>}\;}}1
    %    {:>}{{\code{:>}\;}}1
    {->}{{$\rightarrow\;$}}1
    {<->}{{$\leftrightarrow\;$}}1
    {<==}{{$\leq\;$}}1
    {\#}{{$^\star$}}1 
    {\\o}{{$\circ\;$}}1 
    {\@}{{$\cdot$}}1 
    {\/\\}{{$\wedge\;$}}1
    {\\\/}{{$\vee\;$}}1
    {++}{{\code{++}}}1
    {~}{{\ }}1
    {\@\@}{{$@$}}1
    {\\mapsto}{{$\mapsto\;$}}1
    {\\hline}{{\rule{\linewidth}{0.5pt}}}1
    %
}[keywords,comments,strings]

%%% Citation style
\citestyle{acmauthoryear}

%%% Math settings
\usepackage{amsthm,mathtools}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{lem}{Lemma}[section]
\newtheorem{thm}{Theorem}[section]

%%% PL constructs
\usepackage{galois}
\usepackage{ebproof}
\ebproofset{left label template=\textsc{[\inserttext]}}

%%% Custom commands
\newcommand*{\vbar}{|}
\newcommand*{\finto}{\xrightarrow{\text{\textrm{fin}}}}
\newcommand*{\istype}{\mathrel{â©´}}
\newcommand*{\ortype}{\mathrel{|}}
\newcommand*{\cons}{::}

\def\ovbarw{1.2mu}
\def\ovbarh{1}
\newcommand*{\ovbar}[1]{\mkern \ovbarw\overline{\mkern-\ovbarw{\smash{#1}\scalebox{1}[\ovbarh]{\vphantom{i}}}\mkern-\ovbarw}\mkern \ovbarw}
\newcommand*{\A}[1]{{#1}^{\#}}
\newcommand*{\Expr}{\text{Expr}}
\newcommand*{\ExprVar}{\text{ExprVar}}
\newcommand*{\Module}{\text{Module}}
\newcommand*{\ModVar}{\text{ModVar}}
\newcommand*{\Time}{\mathbb{T}}
\newcommand*{\ATime}{\A{\Time}}
\newcommand*{\Ctx}[1]{\text{Ctx}({#1})}
\newcommand*{\Value}[1]{\text{Val}({#1})}
\newcommand*{\Mem}[1]{\text{Mem}({#1})}
\newcommand*{\mem}{m}
\newcommand*{\AMem}[1]{\A{\text{Mem}}({#1})}
\newcommand*{\State}[1]{\text{State}({#1})}
\newcommand*{\AState}[1]{\A{\text{State}}({#1})}
\newcommand*{\Result}[1]{\text{Result}({#1})}
\newcommand*{\AResult}[1]{\A{\text{Result}}({#1})}
\newcommand*{\link}[2]{{#1}\mathtt{!}{#2}}

\newcommand*{\doubleplus}{\ensuremath{\mathbin{+\mkern-3mu+}}}
\newcommand*{\project}{\text{\texttt{:>} }}
\newcommand*{\EE}{\mathsf{Exp}}
\newcommand*{\LL}{\mathsf{L}}
\newcommand*{\Link}{\mathsf{Link}}
\newcommand*{\sembracket}[1]{\lBrack{#1}\rBrack}
\newcommand*{\fin}[2]{{#1}\xrightarrow{\text{fin}}{#2}}
\newcommand*{\addr}{\mathsf{addr}}
\newcommand*{\tick}{\mathsf{tick}}
\newcommand*{\modctx}{\mathsf{ctx}}
\newcommand*{\mapinject}[2]{{#1}[{#2}]}
\newcommand*{\inject}[2]{{#1}\langle{#2}\rangle}
\newcommand*{\deletepre}[2]{{#2}\overline{\doubleplus}{#1}}
\newcommand*{\deletemap}[2]{{#1}\overline{[{#2}]}}
\newcommand*{\delete}[2]{{#1}\overline{\langle{#2}\rangle}}
\newcommand*{\filter}{\mathsf{filter}}
\newcommand*{\Let}{\mathtt{let}}

\title{A Syntax-Guided Framework for Modular Analysis}
\author{Joonhyup Lee}
\begin{document}
\maketitle

\section{Abstract Syntax}

In this section we define the abstract syntax for a simple language that captures the essence of modules and linking.
The language is basically an extension of untyped lambda calculus with modules and the linking construct.

\begin{figure}[htb]
  \centering
  \footnotesize
  \begin{tabular}{rccll}
    Expression Identifier & $x$ & $\in$         & $\ExprVar$                                   \\
    Module Identifier     & $M$ & $\in$         & $\ModVar$                                    \\
    Expression            & $e$ & $\in$         & $\Expr$                                      \\
    Expression            & $e$ & $\rightarrow$ & $x$                & identifier, expression  \\
                          &     & $\vbar$       & $\lambda x.e$      & function                \\
                          &     & $\vbar$       & $e$ $e$            & application             \\
                          &     & $\vbar$       & $\link{e}{e}$      & linked expression       \\
                          &     & $\vbar$       & $\varepsilon$      & empty module            \\
                          &     & $\vbar$       & $M$                & identifier, module      \\
                          &     & $\vbar$       & $\Let$ $x$ $e$ $e$ & let-binding, expression \\
                          &     & $\vbar$       & $\Let$ $M$ $e$ $e$ & let-binding, module     \\
  \end{tabular}
  \caption{Abstract syntax of the simple module language.}
\end{figure}
\subsection{Rationale for the design of the simple language}

There are no recursive modules, first-class modules, or functors in the simple language that is defined.
Also, note that the nonterminals for the modules and expressions are not separated. Why is this so?

The rationale for the exclusion of recursive modules/first-class modules/functors is because we want to enforce static scoping.
That is, we need to be able to statically determine where variables were bound when using them.
To enforce static scoping when function applications might return modules, we need to employ signatures to project the dynamically computed modules onto a statically known context.
Concretely, we need to define signatures $S$ where $\lambda M\project S.e$ statically resolves the context when $M$ is used in the body $e$, and $(e_1\:e_2)\project S$ enforces that a dynamic computation is resolved into one static form.
To simplify the presentation, we first consider the case that does not require signatures.

The rationale for not separating modules and expressions in the syntax is because we want to utilize the linking construct to link both modules to expressions and modules to modules.
That is, we want expressions to be parsed as $(m_1!m_2)!e$.
$\link{m_1}{m_2}$ links a module with a module, and $(m_1!m_2)!e$ links a module with an expression.
Why this is convenient will be clear when we explain separate analysis; we want to link modules with modules as well as expressions.

\section{Concrete Semantics}

In this section, we present the dynamics of the simple language presented in the previous section.

\subsection{Structural Operational Semantics}

First, we give the big-step operational semantics for the dynamic execution of the module language.
The big-step evaluation relation relates the initial \emph{configuration}(expression and state) with the \emph{result}(value and state) it returns.

Note that the representation of the \emph{environment} that is often used to define closures in the call-by-value dynamics is not simply a finite map from variables to addresses.
Rather, the environment is a stack that records variables \emph{in the order} they were bound.
In the spirit of de Bruijn, to access the value of the variable $x$ from the environment(or the \emph{binding context}) $C$, one has to read off the closest binding time.
Then, the value bound at that time from the memory is read.
Likewise, to access the exported context from the variable $M$, one has to look up the exported context from $C$, not from the memory.

This separation between where we store modules and where we store the evaluated values from expressions emphasizes the fact that \emph{where} the variables are bound is guided by syntax.
The only thing that is dynamic is \emph{when} the variables are bound, which is represented by the time component.

Now, we start by defining what we mean by \emph{time} and \emph{context}, which is the essence of our model.

\subsubsection{Time and Context}

We first define sets that are parametrized by our choice of the time domain, namely the \emph{value}, \emph{memory}, and \emph{context} domains.
Also, we present the notational conventions used in this paper to represent members of each domain.

\begin{figure}[htb]
  \centering
  \footnotesize
  \begin{tabular}{rccll}
    Time                         & $t$    & $\in$         & $\Time$                                                                                                  \\
    Environment/Context          & $C$    & $\in$         & $\Ctx\Time$                                                                                              \\
    Value of expressions         & $v$    & $\in$         & $\Value\Time \triangleq \Expr\times\Ctx\Time$                                                            \\
    Value of expressions/modules & $V$    & $\in$         & $\Value{\Time}+\Ctx{\Time}$                                                                              \\
    Memory                       & $\mem$ & $\in$         & $\Mem{\Time} \triangleq \fin{\Time}{\Value{\Time}}$                                                      \\
    State                        & $s$    & $\in$         & $\State{\Time} \triangleq \Ctx{\Time}\times\Mem{\Time}\times\Time$                                       \\
    Result                       & $r$    & $\in$         & $\Result{\Time} \triangleq (\Value{\Time}+\Ctx{\Time})\times\Mem{\Time}\times\Time$                      \\
    Context                      & $C$    & $\rightarrow$ & []                                                                                  & empty stack        \\
                                 &        & $\vbar$       & $(x,t)\cons C$                                                                      & expression binding \\
                                 &        & $\vbar$       & $(M,C)\cons C$                                                                      & module binding     \\
    Result of expressions        & $v$    & $\rightarrow$ & $\langle \lambda x.e, C \rangle$                                                    & closure
  \end{tabular}
  \caption{Definition of the semantic domains.}
\end{figure}

Above, there are no constraints placed upon the set $\Time$.
Now we give the conditions that the concrete time domain must satisfy.

\begin{definition}[Concrete time]
  $(\Time, \le, \tick)$ is a \emph{concrete time} when
  \begin{enumerate}
    \item $(\Time, \le)$ is a total order.
    \item $\tick : \Ctx{\Time}\rightarrow\Mem{\Time}\rightarrow\Time\rightarrow\ExprVar\rightarrow\Value{\Time}\rightarrow\Time$ satisfies:
          \[\forall t\in\Time: t < \tick\:\_\:\_\:t\:\_\:\_\]
  \end{enumerate}
\end{definition}
The time $\tick\:C\:\mem\:t\:x\:v$ is the time that is incremented when the value $v$ is bound to a variable $x$ at time $t$ under context $C$ and memory $m$.

Why must the $\tick$ function for the concrete time take in $C, m, t, x, v$?
This is for the purpose of program analysis.
For program analysis, the analysis designer must think of an \emph{abstract} tick operator that \emph{simulates} its concrete counterpart.
If the concrete tick function is not able to take into account the environment that the time is incremented, the abstraction of the tick function will not be able to hold much information about the execution of the program.

Now for the auxiliary operators that is used when defining the evaluation relation.
We define the the function that extracts the address for an $\ExprVar$,
and the function that looks up the dynamic context bound to a $\ModVar$ $M$.

\begin{figure}[h!]
  \centering
  \footnotesize
  \[
    \addr(C,x)\triangleq
    \begin{cases}
      \bot         & C=[]                              \\
      t            & C=(x, t)\cons C'                  \\
      \addr(C',x)  & C=(x', t)\cons C' \wedge x'\neq x \\
      \addr(C'',x) & C=(M, C')\cons C''
    \end{cases}
    \quad
    \modctx(C,M)\triangleq
    \begin{cases}
      \bot           & C=[]                               \\
      C'             & C=(M, C')\cons C''                 \\
      \modctx(C'',M) & C=(M', C')\cons C''\wedge M'\neq M \\
      \modctx(C',M)  & C=(x, t)\cons C'
    \end{cases}
  \]
  \caption{Definitions for the $\addr$ and $\modctx$ operators.}
\end{figure}

\subsubsection{The Evaluation Relation}

Now we are in a position to define the big-step evaluation relation.
The relation $\Downarrow$ relates $(e,C,\mem,t)\in\Expr\times\State{\Time}$ with
$(V,\mem,t)\in\Result{\Time}$.
Note that we constrain whether the evaluation relation returns $v\in\Value{\Time}$(when the expression being evaluated is not a module) or $C\in\Ctx{\Time}$ by the definition of the relation.

\begin{figure}[h!]
  \begin{flushright}\fbox{$(e,C,\mem,t)\Downarrow(V,\mem',t')$}\end{flushright}
  \vspace{0pt} % -0.75em}
  \footnotesize
  \[
    \begin{prooftree}
      \hypo{t_{x}=\addr(C,x)}
      \hypo{v=\mem(t_{x})}
      \infer[left label=ExprVar]2{
      (x, C, \mem, t)
      \Downarrow
      (v, \mem, t)
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \infer[left label=Fn]0{
      (\lambda x.e, C, \mem, t)
      \Downarrow
      (\langle\lambda x.e, C\rangle, \mem, t)
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (\langle\lambda x.e_{\lambda}, C_{\lambda}\rangle, \mem_{\lambda}, t_{\lambda}) \\
          (e_{2}, C, \mem_{\lambda}, t_{\lambda})
          \Downarrow
          (v, \mem_{a}, t_{a})                                                            \\
          (e_{\lambda}, (x, t_{a})\cons C_{\lambda}, \mem_{a}[t_{a}\mapsto v], \tick\:C\:\mem_{a}\:t_{a}\:x\:v)
          \Downarrow
          (v', \mem',t')
        \end{matrix}
      }
      \infer[left label={App}]1{
      (e_{1}\:e_{2}, C, \mem, t)
      \Downarrow
      (v', \mem',t')
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (C', \mem', t') \\
          (e_{2}, C', \mem', t')
          \Downarrow
          (V, \mem'', t'')
        \end{matrix}
      }
      \infer[left label=Linking]1{
      (\link{e_{1}}{e_{2}}, C, \mem, t)
      \Downarrow
      (V, \mem'', t'')
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=Empty]0{
      (\varepsilon, C, \mem, t)
      \Downarrow
      (C, \mem, t)
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{C'=\modctx(C,M)}
      \infer[left label=ModVar]1{
      (M, C, \mem, t)
      \Downarrow
      (C', \mem, t)
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (v, \mem', t') \\
          (e_{2}, (x, t')\cons C, \mem'[t'\mapsto v], \tick\:C\:\mem'\:t'\:x\:v)
          \Downarrow
          (C', \mem'', t'')
        \end{matrix}
      }
      \infer[left label=LetE]1{
      (\mathtt{let}\:x\:e_1\:e_2, C, \mem, t)
      \Downarrow
      (C', \mem'', t'')
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (C', \mem', t') \\
          (e_{2}, (M, C')\cons C, \mem', t')
          \Downarrow
          (C'', \mem'', t'')
        \end{matrix}
      }
      \infer[left label=LetM]1{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, C, \mem, t)
      \Downarrow
      (C'', \mem'', t'')
      }
    \end{prooftree}
  \]
  \caption{The concrete big-step evaluation relation.}
  \label{fig:conceval}
\end{figure}
Note that we do not constrain whether $v$ or $C$ is returned by $e_{2}$ in the linking case.
That is, linking may return either values or modules.

The equivalence of the evaluation relation with a reference interpreter is formalized in Coq.

\subsection{Collecting Semantics}

For program analysis, we need to define a collecting semantics that captures the strongest property we want to model.
In the case of modular analysis, we need to collect \emph{all} pairs of $(e,s)\Downarrow r$ that appear in the proof tree when trying to prove what the initial configuration evaluates to.
Consider the case when $\link{e_1}{e_2}$ is evaluated under state $s$.
Since $e_2$ has free variables that are exported by $e_1$, separately analyzing $e_2$ will result in an incomplete proof tree.
What it means to separately analyze, then link two expressions $e_1$ and $e_2$ is to (1) compute what $e_1$ will export to $e_2$ (2) partially compute the proof tree for $e_2$, and (3) inject the exported context into the partial proof to complete the execution of $e_2$.

What should be the \emph{type} of the collecting semantics?
Obviously, given the type of the evaluation relation, $\wp((\Expr\times\State{\Time})\times\Result{\Time})$ seems to be the natural choice.
However, by requiring that all collected pairs have a result fails to collect the configurations that are reached but does not return.
Such a situation will occur frequently when separately analyzing an expression that depends on an external module to resolve its free variables.
Therefore, we extend the relation to relate an element of $\Expr\times\State\Time$ to a \emph{set} of results.
An $(e,s)$ that is not related to any set means that the configuration is not reached, and an $(e,s)$ that is related to an empty set means that the configuration does not return.
This is actually a function from the \emph{reached configurations} during the execution of the program to the set of results the configuration returns.

This ``set of reached configurations'' is intuitively clear.
Given a big-step interpreter that takes in $(e,s)$ and returns $r$, the set of reached configurations is the set that the interpreter looks at during the computation.
To formulate this intuition into a formally correct definition, we need to define a relation defining what configurations are looked at when evaluating the initial configuration.

\subsubsection{Formalizing reachability}

The reachability relation that formalizes the concept of what $(e',s')$ the interpreter ``sees'' when evaluating $(e,s)$ directly follows the definition of the big-step interpreter.
For example, in the case for application, first the function part must be evaluated(AppL), and if the function part is returned, the argument part must be evaluated(AppR), and finally the function body must be evaluated (AppBody).
The complete definition for the reachability relation is given in \ref{fig:concreach}.

\begin{figure}[htb]
  \begin{flushright}\fbox{$(e,C,\mem,t)\rightsquigarrow(e',C',\mem',t')$}\end{flushright}
  \centering
  \footnotesize
  \[
    \begin{prooftree}
      \infer[left label={AppL}]0{
      (e_{1}\:e_{2}, C, \mem, t)
      \rightsquigarrow
      (e_{1},C, \mem,t)
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (\langle\lambda x.e_{\lambda}, C_{\lambda}\rangle, \mem_{\lambda}, t_{\lambda})
        \end{matrix}
      }
      \infer[left label={AppR}]1{
      (e_{1}\:e_{2}, C, \mem, t)
      \rightsquigarrow
      (e_{2}, C, \mem_{\lambda}, t_{\lambda})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (\langle\lambda x.e_{\lambda}, C_{\lambda}\rangle, \mem_{\lambda}, t_{\lambda}) \\
          (e_{2}, C, \mem_{\lambda}, t_{\lambda})
          \Downarrow
          (v, \mem_{a}, t_{a})
        \end{matrix}
      }
      \infer[left label={AppBody}]1{
      (e_{1}\:e_{2}, C, \mem, t)
      \rightsquigarrow
      (e_{\lambda}, (x, t_{a})\cons C_{\lambda}, \mem_{a}[t_{a}\mapsto v], \tick\:C\:\mem_{a}\:t_{a}\:x\:v)
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LinkL]0{
      (\link{e_{1}}{e_{2}}, C, \mem, t)
      \rightsquigarrow
      (e_{1}, C, \mem, t)
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (C', \mem', t')
        \end{matrix}
      }
      \infer[left label=LinkR]1{
      (\link{e_{1}}{e_{2}}, C, \mem, t)
      \rightsquigarrow
      (e_{2}, C', \mem', t')
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LetEL]0{
      (\mathtt{let}\:x\:e_1\:e_2, C, \mem, t)
      \rightsquigarrow
      (e_{1}, C, \mem, t)
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (v, \mem', t')
        \end{matrix}
      }
      \infer[left label=LetER]1{
      (\mathtt{let}\:x\:e_1\:e_2, C, \mem, t)
      \rightsquigarrow
      (e_{2}, (x, t')\cons C, \mem'[t'\mapsto v], \tick\:C\:\mem'\:t'\:x\:v)
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LetML]0{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, C, \mem, t)
      \rightsquigarrow
      (e_{1}, C, \mem, t)
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, C, \mem, t)
          \Downarrow
          (C', \mem', t')
        \end{matrix}
      }
      \infer[left label=LetMR]1{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, C, \mem, t)
      \rightsquigarrow
      (e_{2}, (M, C')\cons C, \mem', t')
      }
    \end{prooftree}
  \]
  \caption{The concrete single-step reachability relation.}
  \label{fig:concreach}
\end{figure}
The well-definedness of the reachability relation with respect to a reference interpreter is formalized in Coq.

\subsubsection{Formulating semantics in terms of a fixpoint}

To define a semantics that is computable, we must formulate the collecting semantics as a least fixed point of a monotonic function that maps an element of some CPO $D$ to $D$.
In our case, elements of the domain $D$ is a cache that remembers what configurations $(e,s)$ the interpreter saw and what results $r$ the interpreter computed, which is common in defining collecting semantics for higher-order languages\cite{HudakYoung,DBLP:journals/pacmpl/DaraisLNH17}.
Mathematically, the cache $a$ is an element of $(\Expr\times\State{\Time})\rightarrow\wp(\Result{\Time})$, when the ordering is defined pointwise, with undefined outputs as the bottom and the rest ordered by the set inclusion order.

Intuitively, such a cache can be interpreted as holding the \emph{intermediate} state of the interpreter.
That is, the cache records \emph{all} intermediate inputs to the interpreter and \emph{all} results that the interpreter computed.
Then the transfer function that takes in a cache and outputs a cache that corresponds to the cache after \emph{one step} of the interpreter becomes clear.
For each reached configurations in the domain of the cache, the transfer function simulates what the actual interpreter will do, assuming that what it knows is constrained by the given cache.
For example, if the function and argument part in an application is already known but the body is yet not computed, the transfer function adds the body of the application to the next cache.


\begin{definition}[Transfer function]
  Given a partial function $a : (\Expr\times\State{\Time})\rightarrow{\wp(\Result{\Time})}$,

  \begin{itemize}
    \item Define ${\Downarrow}_{a}$ and ${\rightsquigarrow}_{a}$ by replacing all premises $(e,s)\Downarrow r$ by $r\in a(e,s)$ in $\Downarrow$ and $\rightsquigarrow$.
    \item Define the ${\mathsf{step}}$ function that does what the interpreter will do with input $(e,s)$ under $a$.
          \[
            \mathsf{step}(a)(e,s)\triangleq
            [(e,s)\mapsto\{r|(e,s){\Downarrow}_{a}r\}]
            \cup
            \bigcup_{(e,s)\rightsquigarrow_{a}(e',s')}[(e',s')\mapsto\varnothing]
          \]
  \end{itemize}

  We define the transfer function $\mathsf{Step}$ by:
  \[
    \mathsf{Step}(a)\triangleq
    \bigcup_{(e,s)\in\mathsf{dom}(a)}
    {\mathsf{step}(a)(e,s)}
  \]
\end{definition}

The $\mathsf{Step}$ function is naturally monotonic, as a cache that knows more will derive more results than a cache that knows less.
Now, because of Tarski's fixpoint theorem, we can formulate the collecting semantics in fixpoint form.
\begin{definition}[Concrete semantics]
  \[
    \sembracket{e}(s)\triangleq\mathsf{lfp}(\lambda a.\mathsf{Step}(a)\cup[(e,s)\mapsto\varnothing])
  \]
\end{definition}

\section{Concrete linking}

To prepare for modular analysis, we need to be able to paste together the semantics of $e_1$ and $e_2$ to obtain the semantics of $\link{e_1}{e_2}$.
Why do we need to define linking on the level of the concrete semantics?
This is because for separate analysis, $e_1$ and $e_2$ must be analyzed in separate abstract time domains $\ATime_1$ and $\ATime_2$, which are sound approximations of \emph{concrete} $\Time_1$ and $\Time_2$ domains.
To elaborate, since the abstract tick must be a sound approximation of the concrete tick, for the analysis to be sound the \emph{initial time} must approximate the concrete initial time.
If a single abstract time and concrete time domain is used, it means that the analysis of $e_2$ must start from an abstract time that approximates the time that $e_1$ exports to $e_2$.
This is against the idea of separate analysis; we want to analyze the behavior of $e_2$ without running $e_1$ beforehand.
Therefore, we need to define a way to link the concrete time domains that allows the analysis of $e_1$ and $e_2$ to approximate the concrete execution without looking at the final time of $e_1$.

Now assume that we have computed $\sembracket{e_1}(C_1,\mem_1,0_1)$ and $\sembracket{e_2}([],\varnothing,0_2)$.
What we want to define is a $\tick_+$ function on $\Time_1\times\Time_2$ that:
\begin{enumerate}
  \item Increments $(0_1,0_2)$ up to $(t_1,0_2)$, when $t_1\in\Time_1$ is the final timestamp of $\sembracket{e_1}(\_,\_,0_1)$.
  \item Increments $(t_1,0_2)$ up to $(t_1,t_2)$, when $t_2\in\Time_2$ is the largest timestamp that can be computed \emph{without} knowing about what $e_1$ exports to $e_2$.
\end{enumerate}

To satisfy the above constraints, the $\tick_+$ function has to use the $\tick_1$ function to increment the first timestamp when the first timestamp is less than $t_1$.
Also, in the case that the first timestamp is greater or equal to $t_1$, the second timestamp is incremented by $\tick_2$ under the context and memory that is \emph{removed} of the exported context.

What does it mean that the exported context should be \emph{removed}?
The second timestamp is assumed to be incremented separately, using $\tick_2$, under the empty context.
However, under the linked time $\Time_1\times\Time_2$, first $(e_1,C_1,\mem_1,(0_1,0_2))\Downarrow(C_2,\mem_2,(t_1,0_2))$ is computed, and only then is $0_2$ incremented under $C_2,\mem_2$.
For $\tick_+$ to produce the same timestamps produced by $\tick_2$ under the empty initial conditions, $C_2$ has to be dug out from the bottom of the stack and $\mem_2$ must be filtered out.
Filtering out $\mem_2$ is easy to do; just ignore addresses with a timestamp different from $t_1$ in the first time component.
Digging out $C_2$ from the stack needs more consideration.

To determine what parts of the stack should be deleted, we need to be able to describe what $C$ will look like if it started from $C_2$, not from $[]$.
That is, we need to determine the injection operator $\inject{C_2}{C}$ that satisfies:
if $(e_2,[],\varnothing,0_2)\rightsquigarrow^*(e,C,\mem,t)$, then $(e_2,C_2,\varnothing,0_2)\rightsquigarrow^*(e,\inject{C_2}{C},\inject{C_2}{\mem},t)$ with $\tick_+$.
The notation $\inject{C_2}{\mem}$ means that the context $C_2$ is injected into all closures in $\mem$.

\begin{figure}[h!]
  \footnotesize
  \[
    \mapinject{C_{1}}{C_{2}}\triangleq
    \begin{cases}
      []                                                 & C_{2}=[]              \\
      (x, t)\cons\mapinject{C_{1}}{C'}                   & C_{2}=(x,t)\cons C'   \\
      (M, \inject{C_{1}}{C'})\cons\mapinject{C_{1}}{C''} & C_{2}=(M,C')\cons C''
    \end{cases}
    \qquad
    \inject{C_{1}}{C_{2}}\triangleq \mapinject{C_{1}}{C_{2}}\doubleplus C_{1}
  \]
  \caption{Definition of the injection operator $\inject{C_1}{C_2}$.}
  \label{fig:concinject}
\end{figure}

The definition for the injection operator in our simple language is more complicated than expected.
This is because when modules are bound to module identifiers, the context that is bound \emph{automatically} includes the exported context.
This is a consequence of the design choice to exclude signatures.
In a more sensible language where signatures designate what variable bindings should be exported, the definition of the injection operator would be simply the list append operator $\doubleplus$.
However, in our language when the injection has to \emph{map} over all module bindings, the injection operator is defined in a mutually recursive manner; one that maps the injection over all bindings($\mapinject{C_1}{C_2}$) and one that actually appends the stacks together($\inject{C_1}{C_2}$).

Now, the definition for the deletion operator that \emph{digs out} the exported context can be naturally defined as the inverse operations of injection.
\begin{figure}[h!]
  \footnotesize
  \[
    \deletepre{C_{1}}{C_{2}}\triangleq
    \begin{cases}
      \deletepre{C_{1}'}{C_{2}'} & (C_{1},C_{2})=(C_{1}'\doubleplus[(x,t)],C_{2}'\doubleplus[(x,t)]) \\
      \deletepre{C_{1}'}{C_{2}'} & (C_{1},C_{2})=(C_{1}'\doubleplus[(M,C)],C_{2}'\doubleplus[(M,C)]) \\
      C_{2}                      & \text{otherwise}
    \end{cases}
  \]

  \[
    \deletemap{C_{1}}{C_{2}}\triangleq
    \begin{cases}
      []                                                 & C_{2}=[]               \\
      (x,t)\cons\deletemap{C_{1}}{C'}                    & C_{2}=(x,t):: C'       \\
      (M, \delete{C_{1}}{C'})\cons\deletemap{C_{1}}{C''} & C_{2}=(M, C')\cons C''
    \end{cases}
    \qquad
    \delete{C_{1}}{C_{2}}\triangleq \deletemap{C_{1}}{\deletepre{C_{1}}{C_{2}}}
  \]
  \caption{Definition of the deletion operators.}
  \label{fig:concdelete}
\end{figure}

The deletion operators satisfy $\deletepre{C_1}{(C_2\doubleplus C_1)}=C_2$, $\deletemap{C_1}{\mapinject{C_1}{C_2}}=C_2$, and $\delete{C_1}{\inject{C_1}{C_2}}=C_2$.

Now only the filter operation has to be defined for the total definition of the $\tick_+$ function.
The filter operation is defined in \ref{fig:concfilter}.
\begin{figure}[h!]
  \footnotesize
  \[
    \filter_1(C)\triangleq
    \begin{cases}
      []                                   & C=[]                              \\
      (x,t.1)\cons\filter_1(C')            & C=(x,t)\cons C'\wedge t.1\neq t_1 \\
      \filter_1(C')                        & C=(x,t)\cons C'\wedge t.1=t_1     \\
      (M,\filter_1(C'))\cons\filter_1(C'') & C=(M, C')\cons C''
    \end{cases}
  \]

  \[
    \filter_2(C)\triangleq
    \begin{cases}
      []                                   & C=[]                              \\
      (x,t.2)\cons\filter_2(C')            & C=(x,t)\cons C'\wedge t.1=t_1     \\
      \filter_2(C')                        & C=(x,t)\cons C'\wedge t.1\neq t_1 \\
      (M,\filter_2(C'))\cons\filter_2(C'') & C=(M, C')\cons C''
    \end{cases}
  \]
  \caption{Definitions for the filter operation.}
  \label{fig:concfilter}
\end{figure}

Note that the linked time domain only uses timestamps of the form $(t_1,0_2)$ and $(t_1,t_2)$.
Therefore, we are actually considering a linked time on a \emph{subset} of $\Time_1\times\Time_2$.
For notational convenience, we write this subset as
\[\Time_1\otimes\Time_2\triangleq\Time_1\times\{0_2\}\cup\{t_1\}\times\Time_2\]
Now the definition of the $\tick_+$ can be given:
\begin{definition}[Concrete linking of time domains]
  Given $(\Time_1,\le_1,\tick_1)$ and $(\Time_2,\le_2,\tick_2)$,

  \begin{itemize}
    \item Let ${s_1}=({C_1},{\mem_1},{t_1})$ be a state in $\Time_1$ with all timestamps in $C_1$ and $m_1$ less than $t_1$.
    \item Define $\le_+$ as the lexicographic order on $\Time_1\times\Time_2$.
    \item Define the ${\tick_{+}}({s_1})$ function as:
          \[
            \tick_{+}({s_1})({C},\mem,{t},x,{v})\triangleq
            \begin{cases}
              (\tick_1\:\filter_1(C,\mem,t,x,v), 0_2)                           & (t.1\neq t_1) \\
              (t_1,\tick_2\:\filter_2(\delete{{(C_1,0_2)}}{C,\mem, {t},x,{v}})) & (t.1=t_1)
            \end{cases}
          \]
          when $(C_1,0_2)$ is $C_1$ with all timestamps lifted to $\Time_1\otimes\Time_2$.
  \end{itemize}

  Then we call the concrete time $(\Time_1\otimes\Time_2,\le_+,\tick_{+}({s_1}))$ the linked time when $s_1$ is exported.
\end{definition}

We can confirm that the linked time domain indeed preserves the timestamps produced by $\tick_2$ without knowledge about the exported state $s_1$.
To formulate this intuition, we need to extend the injection operator to an operator $\rhd$ that injects a configuration from $\Time_1$ to a result in $\Time_2$.
\begin{definition}[Injection of a configuration : $\rhd$]
  $\:$

  Given ${s}=({C_1},{\mem_1},{t_1})\in\State{\Time_1}$ and ${r}=({V_2},{\mem_2},t_2)\in\Result{\Time_2}$,
  define $s\rhd m_2$ and $s\rhd r$ as:
  \[
    s\rhd m_2\triangleq
    \lambda t.
    \begin{cases}
      m_1(t.1)               & t.1\in\mathsf{dom}(m_1)\wedge t.2=0_2 \\
      \inject{C_1}{m_2}(t.2) & t.1=t_1\wedge t.2\in\mathsf{dom}(m_2)
    \end{cases}
    \qquad
    s\rhd r\triangleq
    (\inject{C_1}{V_2},s\rhd m_2,t_2)
  \]
  assuming that all timestamps $t\in\Time_1$ is lifted to $(t,0_2)$ and all timestamps $t\in\Time_2$ is lifted to $(t_1,t)$.

  We extend the $\rhd$ operator to inject ${s}$ in a cache $a\in(\Expr\times\State{\Time_2})\rightarrow{\wp(\Result{\Time_2})}$:
  \[
    {s}\rhd{a}\triangleq\bigcup_{(e,{s'})\in\mathsf{dom}({a})}[(e,{s}\rhd{s'})\mapsto\{{s}\rhd{r}|{r}\in{a}(e,{s'})\}]
  \]
\end{definition}

Then we can prove that the $\tick_+$ function is indeed \emph{well-defined}.

\begin{lem}[Injection preserves timestamps under linked time]
  \[
    \forall s\in\State{\Time_1},s'\in\State{\Time_2}:{s}\rhd{\sembracket{e}}({s'})\sqsubseteq{\sembracket{e}}({s}\rhd{s'})
  \]
\end{lem}

Now, as promised in the start of this section, we present how to link the semantics to obtain the semantics under linked time.

\begin{definition}[Auxiliary operators for concrete linking]
  \begin{align*}
    \EE\:e_1\:s        & \triangleq\sembracket{e_1}(s)(e_1,s)                                    & (\text{Exported under }s)      \\
    \LL\:E\:e_2        & \triangleq\bigcup_{s'\in E}\sembracket{e_2}(s'\rhd 0_2)                 & (\text{Reached under }E)       \\
    \mathsf{F}\:E\:e_2 & \triangleq\bigcup_{s'\in E}\sembracket{e_2}(s'\rhd 0_2)(e_2,s'\rhd 0_2) & (\text{Final results under }E)
  \end{align*}
\end{definition}

The intuition is, when linking $e_1$ and $e_2$ under initial configuration $s$, first $e_1$ is computed, then exports($\EE$) its results to $e_2$, which $e_2$ is linked($\LL$) with.
The final result for the total expression $\link{e_1}{e_2}$ will be the final result($\mathsf{F}$) of $e_2$ under the exported context.

\begin{definition}[Concrete linking operator]
  \begin{align*}
    \Link\:e_1\:e_2\:s & \triangleq\sembracket{e_1}(s)\cup\LL\:(\EE\:e_1\:s)\:e_2\cup[(\link{e_1}{e_2},s)\mapsto\mathsf{F}\:(\EE\:e_1\:s)\:e_2]
  \end{align*}

  When all timestamps $t\in\Time_1$ are lifted to $(t,0_2)$.
\end{definition}

Then the following result follows directly from the \emph{definition} of the collecting semantics.

\begin{thm}[Concrete linking]
  \[
    \sembracket{\link{e_1}{e_2}}(s,0_2)=\Link\:e_1\:e_2\:s
  \]
\end{thm}

\section{Abstract Semantics}

The abstract semantics is almost exactly the same as the concrete semantics, except for the fact that the memory domain is now a finite map from the abstract time domain to a \emph{set} of values.
Note we do not need to define the $\A{C}$, $\A{v}$, $\A{V}$ components, as they are \emph{exactly} their concrete counterparts.
They are simply $C$, $v$, $V$, parametrized by a different $\Time$.

\begin{figure}[h!]
  \centering
  \footnotesize
  \begin{tabular}{rccll}
    Abstract Time                & $\A{t}$  & $\in$ & $\ATime$                                                                                   \\
    Environment/Context          & $\A{C}$  & $\in$ & $\Ctx\ATime$                                                                               \\
    Value of expressions         & $\A{v}$  & $\in$ & $\Value\ATime$                                                                             \\
    Value of expressions/modules & $\A{V}$  & $\in$ & $\Value{\ATime}+\Ctx{\ATime}$                                                              \\
    Abstract Memory              & $\A\mem$ & $\in$ & $\AMem{\ATime} \triangleq \fin{\ATime}{\wp(\Value{\ATime})}$                               \\
    Abstract State               & $\A{s}$  & $\in$ & $\AState{\ATime} \triangleq \Ctx{\ATime}\times\Mem{\ATime}\times\ATime$                    \\
    Abstract Result              & $\A{r}$  & $\in$ & $\AResult{\ATime} \triangleq (\Value{\ATime}+\Ctx{\ATime})\times\AMem{\ATime}\times\ATime$ \\
  \end{tabular}
  \caption{Definition of the semantic domains.}
\end{figure}

First the abstract evaluation relation $\A{\Downarrow}$ is defined.
Note that the update for the memory is now a weak update. That is,
\begin{definition}[Weak update]
  Given $\A{\mem}\in\AMem{\ATime}$, $\A{t}\in\ATime$, $\A{v}\in\Value{\ATime}$, we define $\A{\mem}[\A{t}\A{\mapsto}\A{v}]$ as:
  \[
    \A{\mem}[\A{t}\A{\mapsto}\A{v}](\A{t'})\triangleq
    \begin{cases}
      \A{\mem}(\A{t})\cup\{\A{v}\} & (\A{t'}=\A{t})     \\
      \A{\mem}(\A{t'})             & (\text{otherwise})
    \end{cases}
  \]
\end{definition}

Also, for the abstract time, we do not enforce the existence of an ordering on the timestamps, but we do need a policy for performing the tick operation.
The abstract $\A\tick$ must simulate the $\tick$ function, so it must have the same type as $\tick$.
\begin{definition}[Abstract time]
  $(\ATime,\A{\tick})$ is an \emph{abstract time} when $\A{\tick}:\Ctx{\ATime}\rightarrow\AMem{\ATime}\rightarrow\ATime\rightarrow\ExprVar\rightarrow\Value{\ATime}\rightarrow\ATime$ is the policy for advancing the timestamp.
\end{definition}

The abstract big-step evaluation relation is defined in \ref{fig:abseval}, and the single-step reachability relation is defined in \ref{fig:absreach}.

\begin{figure}[h!]
  \begin{flushright}\fbox{$(e,\A{C},\A\mem,\A{t})\A\Downarrow(\A{V},\A{\mem'},\A{t'})$}\end{flushright}
  \footnotesize
  \[
    \begin{prooftree}
      \hypo{\A{t_{x}}=\addr(\A{C},x)}
      \hypo{\A{v}\in\A{\mem}(\A{t_{x}})}
      \infer[left label=ExprVar]2{
      (x, \A{C}, \A{\mem}, \A{t})
      \A{\Downarrow}
      (\A{v}, \A{\mem}, \A{t})
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \infer[left label=Fn]0{
      (\lambda x.e, \A{C}, \A{\mem}, \A{t})
      \A{\Downarrow}
      (\langle\lambda x.e, \A{C}\rangle, \A{\mem}, \A{t})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A{\Downarrow}
          (\langle\lambda x.e_{\lambda}, \A{C_{\lambda}}\rangle, \A{\mem_{\lambda}}, \A{t_{\lambda}}) \\
          (e_{2}, \A{C}, \A{\mem_{\lambda}}, \A{t_{\lambda}})
          \A{\Downarrow}
          (\A{v}, \A{\mem_{a}}, \A{t_{a}})                                                            \\
          (e_{\lambda}, (x, \A{t_a})\cons \A{C_{\lambda}}, \A{\mem_{a}}[\A{t_{a}}\A{\mapsto} \A{v}], \A{\tick}\:\A{C}\:\A{\mem_a}\:\A{t_{a}}\:x\:\A{v})
          \A{\Downarrow}
          (\A{v'}, \A{\mem'},\A{t'})
        \end{matrix}
      }
      \infer[left label={App}]1{
      (e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A{\Downarrow}
      (\A{v'}, \A{\mem'},\A{t'})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A{\Downarrow}
          (\A{C'}, \A{\mem'}, \A{t'}) \\
          (e_{2}, \A{C'}, \A{\mem'}, \A{t'})
          \A{\Downarrow}
          (\A{V}, \A{\mem''}, \A{t''})
        \end{matrix}
      }
      \infer[left label=Linking]1{
      (\link{e_{1}}{e_{2}}, \A{C}, \A{\mem}, \A{t})
      \A\Downarrow
      (\A{V}, \A{\mem''}, \A{t''})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=Empty]0{
      (\varepsilon, \A{C}, \A{\mem}, \A{t})
      \A{\Downarrow}
      (\A{C}, \A{\mem}, \A{t})
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{\A{C'}=\modctx(\A{C},M)}
      \infer[left label=ModVar]1{
      (M, \A{C}, \A{\mem}, \A{t})
      \A{\Downarrow}
      (\A{C'}, \A{\mem}, \A{t})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A\Downarrow
          (\A{v}, \A{\mem'}, \A{t'}) \\
          (e_{2}, (x, \A{t'})\cons \A{C}, \A{\mem'}[\A{t'}\A{\mapsto} \A{v}], \A{\tick}\:\A{C}\:\A{\mem'}\:\A{t'}\:x\:\A{v})
          \A\Downarrow
          (\A{C'}, \A{\mem''}, \A{t''})
        \end{matrix}
      }
      \infer[left label=LetE]1{
      (\mathtt{let}\:x\:e_1\:e_2, \A{C}, \A{\mem}, \A{t})
      \A\Downarrow
      (\A{C'}, \A{\mem''}, \A{t''})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A\Downarrow
          (\A{C'}, \A{\mem'}, \A{t'}) \\
          (e_{2}, (M, \A{C'})\cons\A{C}, \A{\mem'}, \A{t'})
          \A\Downarrow
          (\A{C''}, \A{\mem''}, \A{t''})
        \end{matrix}
      }
      \infer[left label=LetM]1{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\Downarrow
      (\A{C''}, \A{\mem''}, \A{t''})
      }
    \end{prooftree}
  \]
  \caption{The abstract big-step evaluation relation.}
  \label{fig:abseval}
\end{figure}

\begin{figure}[h!]
  \begin{flushright}\fbox{$(e,\A{C},\A\mem,\A{t})\A\rightsquigarrow(e',\A{C'},\A{\mem'},\A{t'})$}\end{flushright}
  \centering
  \footnotesize
  \[
    \begin{prooftree}
      \infer[left label={AppL}]0{
      (e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{1}, \A{C}, \A{\mem}, \A{t})
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A{\Downarrow}
          (\langle\lambda x.e_{\lambda}, \A{C_{\lambda}}\rangle, \A{\mem_{\lambda}}, \A{t_{\lambda}})
        \end{matrix}
      }
      \infer[left label={AppR}]1{
      (e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{2}, \A{C}, \A{\mem_{\lambda}}, \A{t_{\lambda}})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A{\Downarrow}
          (\langle\lambda x.e_{\lambda}, \A{C_{\lambda}}\rangle, \A{\mem_{\lambda}}, \A{t_{\lambda}}) \\
          (e_{2}, \A{C}, \A{\mem_{\lambda}}, \A{t_{\lambda}})
          \A{\Downarrow}
          (\A{v}, \A{\mem_{a}}, \A{t_{a}})
        \end{matrix}
      }
      \infer[left label={AppBody}]1{
      (e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{\lambda}, (x, \A{t_a})\cons \A{C_{\lambda}}, \A{\mem_{a}}[\A{t_{a}}\A{\mapsto} \A{v}], \A{\tick}\:\A{C}\:\A{\mem_a}\:\A{t_{a}}\:x\:\A{v})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LinkL]0{
      (\link{e_{1}}{e_{2}}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{1}, \A{C}, \A{\mem}, \A{t})
      }
    \end{prooftree}\qquad
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A{\Downarrow}
          (\A{C'}, \A{\mem'}, \A{t'})
        \end{matrix}
      }
      \infer[left label=LinkR]1{
      (\link{e_{1}}{e_{2}}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{2}, \A{C'}, \A{\mem'}, \A{t'})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LetEL]0{
      (\mathtt{let}\:x\:e_1\:e_2, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{1}, \A{C}, \A{\mem}, \A{t})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A\Downarrow
          (\A{v}, \A{\mem'}, \A{t'})
        \end{matrix}
      }
      \infer[left label=LetER]1{
      (\mathtt{let}\:x\:e_1\:e_2, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{2}, (x, \A{t'})\cons\A{C}, \A{\mem'}[\A{t'}\A{\mapsto} \A{v}], \A{\tick}\:\A{C}\:\A{\mem'}\:\A{t'}\:x\:\A{v})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \infer[left label=LetML]0{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{1}, \A{C}, \A{\mem}, \A{t})
      }
    \end{prooftree}
  \]

  \[
    \begin{prooftree}
      \hypo{
        \begin{matrix}
          (e_{1}, \A{C}, \A{\mem}, \A{t})
          \A\Downarrow
          (\A{C'}, \A{\mem'}, \A{t'})
        \end{matrix}
      }
      \infer[left label=LetMR]1{
      (\mathtt{let}\:M\:e_{1}\:e_{2}, \A{C}, \A{\mem}, \A{t})
      \A\rightsquigarrow
      (e_{2}, (M,\A{C'})\cons\A{C}, \A{\mem'}, \A{t'})
      }
    \end{prooftree}
  \]
  \caption{The abstract single-step reachability relation.}
  \label{fig:absreach}
\end{figure}

From the relations, we can define the abstract semantics in the same way as the concrete version:

\begin{definition}[Transfer function]
  Given a cache $\A{a}$ in $(\Expr\times\AState{\ATime})\rightarrow{\wp(\AResult{\ATime})}$,

  \begin{itemize}
    \item Define $\A{\Downarrow}_{\A{a}}$ and $\A{\rightsquigarrow}_{\A{a}}$ by replacing all premises $(e,\A{s})\A\Downarrow\A{r}$ by $\A{r}\in\A{a}(e,\A{s})$ in $\A\Downarrow$ and $\A\rightsquigarrow$.
    \item Define the $\A{\mathsf{step}}$ function that collects all results derivable in one step from $(e,\A{s})$ using $\A{a}$.
          \[
            \A{\mathsf{step}}(\A{a})(e,\A{s})\triangleq
            [(e,\A{s})\mapsto\{\A{r}|(e,\A{s})\A{\Downarrow}_{\A{a}}\A{r}\}]
            \cup
            \bigcup_{(e,\A{s})\A{\rightsquigarrow}_{\A{a}}(e',\A{s'})}[(e',\A{s'})\mapsto\varnothing]
          \]
  \end{itemize}

  We define the transfer function $\A{\mathsf{Step}}$ by:
  \[
    \A{\mathsf{Step}}(\A{a})\triangleq
    \bigcup_{(e,\A{s})\in\mathsf{dom}(\A{a})}
    {\A{\mathsf{step}}(\A{a})(e,\A{s})}
  \]
\end{definition}

\begin{definition}[Abstract semantics]
  \[
    \A{\sembracket{e}}(\A{s})\triangleq\mathsf{lfp}(\lambda \A{a}.\A{\mathsf{Step}}(\A{a})\cup[(e,\A{s})\mapsto\varnothing])
  \]
\end{definition}

\section{Whole-program Analysis}
This section clarifies what we mean by that the abstract semantics is a \emph{sound approximation} of the concrete semantics.
Since the only values in our language are closures that pair code with a context, we can make a Galois connection between $\wp(\Result{\Time})$ and $\wp(\AResult{\ATime})$ given a function $\alpha:\Time\rightarrow\ATime$.

\begin{definition}[Extensions of abstraction]
  Given a function $\alpha:\Time\rightarrow\ATime$,
  \begin{itemize}
    \item Extend $\alpha$ to a function on $\Ctx{\Time}\rightarrow\Ctx{\ATime}$ by mapping $\alpha$ over all timestamps.
    \item Extend $\alpha$ to a function on $\Value{\Time}\rightarrow\Value{\ATime}$ by mapping $\alpha$ over all timestamps.
    \item Extend $\alpha$ to a function on $\Mem{\Time}\rightarrow\AMem{\ATime}$ by defining
          \[\alpha(\mem)\triangleq\bigcup_{t\in\mathsf{dom}(\mem)}[\alpha(t)\mapsto\{\alpha(\mem(t))\}]\]
    \item Extend $\alpha$ to a function on $\Result{\Time}\rightarrow\AResult{\ATime}$ by $\alpha(V,\mem,t)\triangleq(\alpha(V),\alpha(\mem),\alpha(t))$
  \end{itemize}
\end{definition}

Then it is obvious that:
\begin{lem}[Galois connection]
  Given a function $\alpha:\Time\rightarrow\ATime$,
  \begin{itemize}
    \item Extend ${\alpha}:\wp(\Result{\Time})\rightarrow\wp(\AResult{\ATime})$ by ${\alpha}(R)\triangleq\{\alpha(r)|r\in R\}$.
    \item Define ${\gamma}:\wp(\AResult{\ATime})\rightarrow\wp(\Result{\Time})$ by ${\gamma}(\A{R})\triangleq\{r|\alpha(r)\in\A{R}\}$.
  \end{itemize}
  Then $\forall R\subseteq\Result{\Time},\A{R}\subseteq\AResult{\ATime}:{\alpha}(R)\subseteq\A{R}\Leftrightarrow R\subseteq{\gamma}(\A{R})$.
\end{lem}

The ordering between elements of $\wp(\Result{\Time})$ and $\wp(\AResult{\ATime})$ is the subset order, because currently the only thing we are abstracting is the \emph{time} component, which describes the control flow of the program.
That is, the abstract semantics can be viewed as a control flow graph of the program, with the notion of ``program points'' described by the pair $(e,\A{s})$.

This Galois connection can naturally be extended to a connection between the abstract and concrete caches, when the abstraction $\alpha(a)$ of a cache $a$ is defined by
\[\alpha(a)\triangleq\bigcup_{(e,s)\in\mathsf{dom}(a)}[(e,\alpha(s))\mapsto\alpha(a(e,s))]\]
and the concretization $\gamma$ is defined by
\[\gamma(\A{a})\triangleq\bigcup_{(e,\A{s})\in\mathsf{dom}(\A{a})}\bigcup_{s\in\gamma(\{\A{s}\})}[(e,s)\mapsto\gamma(\A{a}(e,\A{s}))]\].
Then all we need to show is that the abstract semantics overapproximates the concrete semantics, i.e, that $\sembracket{e}(s)\sqsubseteq\gamma(\A{\sembracket{e}}(\alpha(s)))$.
However, it is not the case that this holds for arbitrary $\alpha$.
It must be that, as emphasized constantly in the previous sections, that $\A\tick$ is a sound approximation of $\tick$ with respect to $\alpha$.

\begin{definition}[Tick-approximating abstraction]
  Given a concrete time $(\Time,\le,\tick)$ and an abstract time $(\ATime, \A\tick)$, a function $\alpha:\Time\rightarrow\ATime$ is said to be \emph{tick-approximating} if:
  \[
    \forall C,\mem,x,t,v:\alpha(\tick\:C\:\mem\:x\:t\:v)=\A\tick\:\alpha(C)\:\alpha(\mem)\:x\:\alpha(t)\:\alpha(v)
  \]
\end{definition}

Now we can prove that:
\begin{lem}[Soundness]
  Given a tick-approximating $\alpha:\Time\rightarrow\ATime$,
  \[
    \forall s\in\State{\Time}:\sembracket{e}(s)\sqsubseteq\gamma(\A{\sembracket{e}}(\alpha(s)))
  \]
\end{lem}

What's not obvious is that if the abstract time domain is finite, the analysis is guaranteed to terminate.
Since bindings for modules also exist in the stack $C$, showing that the state space given a finite $\ATime$ is finite is nontrivial.
However, since the \emph{syntax} of the program constrains how $C$ looks like, we can prove that:

\begin{thm}[Finiteness of time implies finiteness of abstraction]
  If $\ATime$ is finite,
  \[
    \forall e,\A{s}: \A{\sembracket{e}}(\A{s})\text{ can be finitely computed.}
  \]
\end{thm}

\section{Separate Analysis}

\subsection{Addition of time domains}
For separate analysis, we need to define the linking operators in a way that soundly approximates the concrete version of linking.
Note that in concrete linking, the time domains were linked based on the fact that the timestamps are ordered by a total order.
Remember that the filtering operation determined whether the timestamp came \emph{before} or \emph{after} linking by comparing the first time component with the \emph{final} time before linking.
In the abstract semantics, such an approach is impossible, since the abstract timestamps do not preserve the order of the concrete timestamps.
Thus, in the abstract semantics, the linked timestamp must live in $\ATime_1+\ATime_2$.
The intuition is that the timestamps before linking and after linking is determined by their membership in each time domain.

Then the filtering operation for the context can naturally be defined as in \ref{fig:absfilter}, and the definition for the added time domain can be given.
\begin{figure}[h!]
  \footnotesize
  \[
    \A\filter(\A{C},\ATime)\triangleq
    \begin{cases}
      []                                                         & \A{C}=[]                                             \\
      (x,\A{t})\cons\A\filter(\A{C'},\ATime)                     & \A{C}=(x,\A{t})\cons \A{C'}\wedge \A{t}\in\ATime     \\
      \A\filter(\A{C'},\ATime)                                   & \A{C}=(x,\A{t})\cons \A{C'}\wedge \A{t}\not\in\ATime \\
      (M,\A\filter(\A{C'},\ATime))\cons\A\filter(\A{C''},\ATime) & \A{C}=(M, \A{C'})\cons \A{C''}
    \end{cases}
  \]
  \caption{Definition of the abstract filter operation.}
  \label{fig:absfilter}
\end{figure}

\begin{definition}[Addition of time domains]
  Given $(\ATime_1,\A\tick_1)$ and $(\ATime_2,\A\tick_2)$,

  \begin{itemize}
    \item Let $\A{s}_1=(\A{C}_1,\A{\mem}_1,\A{t}_1)$ be a state in $\ATime_1$.
    \item Define the $\A{\tick}_+(\A{s}_1)$ function as:
          \[
            \A\tick_{+}(\A{s}_1)(\A{C},\A\mem,\A{t},x,\A{v})\triangleq
            \begin{cases}
              \A{\tick}_1\:\A\filter((\A{C},\A\mem,\A{t},x,\A{v}),\ATime_1)                 & (\A{t}\in\ATime_1) \\
              \A{\tick}_2\:\A\filter(\delete{\A{C}_1}{\A{C},\A\mem,\A{t},x,\A{v}},\ATime_2) & (\A{t}\in\ATime_2)
            \end{cases}
          \]
  \end{itemize}

  Then we call the abstract time $(\ATime_1+\ATime_2,\A\tick_{+}(\A{s}_1))$ the linked time when $\A{s}_1$ is exported.
\end{definition}

Now the rest flows analogously to concrete linking.
First the injection operator that injects the exported state to the next time must be defined.

\begin{definition}[Injection of a configuration : $\A\rhd$]
  $\:$

  Given $\A{s}=(\A{C}_1,\A{\mem}_1,\A{f}_1)\in\AState{\ATime_1}$ and $\A{r}=(\A{V}_2,\A{\mem}_2,\A{t}_2)\in\AResult{\ATime_2}$,
  let $\A{s}\A\rhd \A{\mem}_2$ and $\A{s}\A\rhd \A{r}$:
  \[
    \A{s}\A\rhd\A{\mem}_2\triangleq
    \lambda \A{t}.
    \begin{cases}
      \A{\mem}_1(\A{t})                   & \A{t}\in\ATime_1 \\
      \inject{\A{C}_1}{\A{\mem}_2}(\A{t}) & \A{t}\in\ATime_2
    \end{cases}
    \qquad
    \A{s}\A\rhd \A{r}\triangleq
    (\inject{\A{C}_1}{\A{V}_2},\A{s}\A\rhd \A{\mem}_2,\A{t}_2)
  \]

  We extend the $\A\rhd$ operator to inject $\A{s}$ in a cache $\A{a}\in(\Expr\times\AState{\ATime_2})\rightarrow{\wp(\AResult{\ATime_2})}$:
  \[
    \A{s}\A\rhd\A{a}\triangleq\bigcup_{(e,\A{s'})\in\mathsf{dom}(\A{a})}[(e,\A{s}\A\rhd\A{s'})\mapsto\{\A{s}\A\rhd\A{r}|\A{r}\in\A{a}(e,\A{s'})\}]
  \]
\end{definition}

Then in the same manner as concrete linking, we have that:

\begin{lem}[Injection preserves timestamps under added time]
  \[
    \forall \A{s}\in\AState{\ATime_1},\A{s'}\in\AState{\ATime_2}:\A{s}\A\rhd{\A{\sembracket{e}}}(\A{s'})\sqsubseteq\A{\sembracket{e}}(\A{s}\A\rhd\A{s'})
  \]
\end{lem}

We must also define the addition operator that recovers the semantics of the linked expression $e_2$ from the exported state $\A{s_1}$ and the \emph{separately} analyzed semantics of $e_2$.

\begin{definition}[Addition between exported configurations and separately analyzed results]
  $\:$

  Let $\A{s}_1$ be a configuration in $\ATime_1$, and let $\A{a}_2=\A{\sembracket{e}}(\A{s'})$ be the semantics of $e$ under $(\ATime_2,\A\tick_2)$.
  Define the ``addition'' between $\A{s}_1$ and $\A{a}_2$ as:
  \[
    \A{s}_1\oplus\A{a}_2\triangleq\mathsf{lfp}(\lambda\A{a}.\A{\mathsf{Step}}(\A{a})\cup(\A{s}_1\A\rhd\A{a}_2))
  \]
\end{definition}

Then because of the previous lemma, it is obvious that:
\begin{lem}[Addition of semantics equals semantics under added time]
  \[
    \A{s}\oplus\A{\sembracket{e}}(\A{s'}) = \A{\sembracket{e}}(\A{s}\A\rhd\A{s'})
  \]
\end{lem}

\subsection{Separating soundness}
The only thing that remains is the formulation of soundness between $\sembracket{\link{e_1}{e_2}}(s)$ under the linked time $(\Time_1\otimes\Time_2,\le_+,\tick_{+}(s_1))$, when $s_1$ is the exported context, and the abstract semantics.

The tricky part is in the time $(t_1,0_2)$.
It is represented by \emph{both} $\alpha_1(t_1)\in\ATime_1$ and $\alpha_2(0_2)\in\ATime_2$, when $\alpha_1:\Time_1\rightarrow\ATime_1$ and $\alpha_2:\Time_2\rightarrow\ATime_2$ are tick-approximating.
Therefore, we cannot make a tick-approximating function between $\Time_1\otimes\Time_2$ and $\ATime_1+\ATime_2$.
Instead, we define a function $\alpha_+:\Time_1\otimes\Time_2\rightarrow\ATime_1+\ATime_2$ by using $\alpha_1$ and $\alpha_2$ which is not tick-approximating on the whole domain but is sound for all timestamps $t=(t_1,\_)$.
That is, we will define $\alpha_+$ so that the following holds:
\[
  \forall t\in\Time_2,C,\mem,x,v:\alpha_+(\tick_+\:C\:\mem\:(t_1,t)\:x\:v)=\A\tick_+\:\alpha_+(C)\:\alpha_+(\mem)\:\alpha_+(t_1,t)\:x\:\alpha_+(v)
\]

Fortunately, such an $\alpha_+$ is easy to find.
\begin{lem}[Linked abstraction]
  Let $s_1=(C_1,\mem_1,t_1)\in\State{\Time_1}$, and let $\alpha_1:\Time_1\rightarrow\ATime_1$. Also,

  \begin{itemize}
    \item Let all timestamps in $C_1,\mem_1$ be less than $t_1$.
    \item Let $\alpha_2:\Time_2\rightarrow\ATime_2$ be a tick-approximating abstraction.
    \item Define $\alpha_+:\Time_1\otimes\Time_2\rightarrow\ATime_1+\ATime_2$ as:
          \[
            \alpha_+(t)\triangleq
            \begin{cases}
              \alpha_1(t.1) & (t.1\neq t_1) \\
              \alpha_2(t.2) & (t.1=t_1)
            \end{cases}
          \]
  \end{itemize}
  Then $\alpha_+$ is tick-approximating on $t=(t_1,\_)$ for $(\Time_1\otimes\Time_2,\le_+,\tick_+(s_1))$ and $(\ATime_1+\ATime_2,\A\tick_+(\alpha_1(s_1)))$.
\end{lem}

Since we gave up tick-approximation for the times before linking, we need to \emph{separate} the problem of finding a sound approximation of $\sembracket{e_1}(s)$ and finding a sound approximation of $\LL\:E\:e_2$.

Finding a sound approximation of $\sembracket{e_1}(s)$ is easy.
From the results of the previous section, if we have a tick-approximating $\alpha_1$ between $\Time_1$ and $\ATime_1$, $\A{\sembracket{e_1}}(\alpha_1(s))$ is automatically a sound approximation.
The problem of finding a sound approximation for $\LL\:E\:e_2$ is also easy if we have a sound approximation $\A{E}$ of $E$ that satisfies $\alpha_1(E)\subseteq\A{E}$.
Since $\alpha_1(s)\in\A{E}$ for all $s\in E$, if we merge $\A{s'}\oplus\A{\sembracket{e_2}}(\A{0}_2)$ for all $\A{s'}\in\A{E}$, $\alpha_+(\LL\:E\:e_2)$ will be contained in the merged cache.
That is, if we define:
\begin{definition}[Auxiliary operators for abstract linking]
  \begin{align*}
    \A\EE\:e_1\:\A{s}          & \triangleq\A{\sembracket{e_1}}(\A{s})(e_1,\A{s})                                                    & (\text{Exported under }\A{s})      \\
    \A\LL\:\A{E}\:e_2          & \triangleq\bigcup_{\A{s'}\in \A{E}}(\A{s'}\oplus\A{\sembracket{e_2}}(\A{0}_2))                      & (\text{Reached under }\A{E})       \\
    \A{\mathsf{F}}\:\A{E}\:e_2 & \triangleq\bigcup_{\A{s'}\in \A{E}}(\A{s'}\oplus\A{\sembracket{e_2}}(\A{0}_2))(\A{s'}\A\rhd\A{0}_2) & (\text{Final results under }\A{E})
  \end{align*}
\end{definition}

Then we have that:
\begin{lem}[Separation of soundness]
  Given $E\subseteq\State{\Time_1}$ and $\A{E}\subseteq\AState{\ATime_1}$, assume:
  \begin{itemize}
    \item There exists an $\alpha_1:\Time_1\rightarrow\ATime_1$ satisfying $\alpha_1(E)\subseteq\A{E}$.
    \item There exists a time-approximating $\alpha_2:\Time_2\rightarrow\ATime_2$.
  \end{itemize}

  Then $\alpha_+(\LL\:E\:e_2)\sqsubseteq\A\LL\:\A{E}\:e_2$ and $\alpha_+(\mathsf{F}\:E\:e_2)\subseteq\A{\mathsf{F}}\:\A{E}\:e_2$.
\end{lem}

\subsection{Soundness of separate analysis}
Now, we may define the abstract linking operator that soundly approximates the concrete linking operator.
\begin{definition}[Abstract linking operator]
  \[\A\Link\:e_1\:e_2\:\A{s}\triangleq\A{\sembracket{e_1}}(\A{s})\cup\A\LL\:(\A\EE\:e_1\:\A{s})\:e_2\cup[(\link{e_1}{e_2},\A{s})\mapsto\A{\mathsf{F}}\:(\A\EE\:e_1\:\A{s})\:e_2]\]
\end{definition}
We want to show that the abstract linking operation is a sound approximation of concrete linking.
However, as emphasized in the previous subsection, the statement of soundness cannot be achieved through just a single concretization function.
Since abstract linking approximates its concrete counterpart \emph{separately}, we need to concretize the part \emph{before} linking and \emph{after} linking separately.

\begin{thm}[Abstract linking]
  Let $\Time_i(i=1,2)$ be two concrete times, and let $\ATime_i(i=1,2)$ be two abstract times.
  Let $\alpha_i:\Time_i\rightarrow\ATime_i(i=1,2)$ be tick-approximating, and let $\A{s}=\alpha_1(s)$ approximate the initial state.
  Then, $\A\Link\:e_1\:e_2\:\A{s}$ is a sound approximation of $\Link\:e_1\:e_2\:s$.
  That is:
  \[\Link\:e_1\:e_2\:s\sqsubseteq\gamma_1(\A{\sembracket{e_1}}(\A{s}))\cup\gamma_+(\A\LL\:(\A\EE\:e_1\:\A{s})\:e_2\cup[(\link{e_1}{e_2},\A{s})\mapsto\A{\mathsf{F}}\:(\A\EE\:e_1\:\A{s})\:e_2])\]
  when the Galois pairs of $\alpha_1$ and $\alpha_+$, $\gamma_1$ and $\gamma_+$, are defined as in section 5.
\end{thm}

All is fine for linking two expressions.
The approximation for the exporting expression comes directly from the abstract semantics, and the approximation for the importing expression comes from linking the exporting set with the separately analyzed results.
However, the above theorem is not strong enough for linking more than two expressions.
This is because $\A\Link\:e_1\:e_2\:\A{s}$ does \emph{not} equal $\A{\sembracket{\link{e_1}{e_2}}}(\A{s})$, as $\A\tick_+$ cannot leap between $\ATime_1$ and $\ATime_2$.
Thus, $\A\Link\:\link{e_1}{e_2}\:e_3\:\A{s}$ does not mean that the semantics for the exporting expression is computed separately.
Also, $\A\Link\:e_1\:\link{e_2}{e_3}\:\A{s}$ does not help much, since computing $\A{\sembracket{\link{e_2}{e_3}}}(\A{0})$ will be stuck before even reaching $e_3$.
To clarify on how to link an \emph{arbitrary} number of modules, we state the following theorem:
\begin{thm}[Compositionality]
  Given a sequence $\{e_n\}_{n\ge 0}$ and initial condition $s\in\State{\Time_0}$,

  \begin{itemize}
    \item Let $\ATime_n$ be abstract times connected with the concrete times by tick-approximating $\alpha_n$.
    \item Let the linked expressions $l_n$ be $l_0\triangleq e_0$, $l_{n+1}\triangleq\link{l_n}{e_{n+1}}$, and let $t_n$ be the final time of $\sembracket{l_n}(s)$.
    \item Define the linked abstraction functions $\alpha^{n}_+$ as:
          \[
            \alpha^0_+\triangleq\alpha_0
            \qquad
            \alpha^{n+1}_+(t)\triangleq
            \begin{cases}
              \alpha^n_+(t.1)   & (t.1\neq t_n) \\
              \alpha_{n+1}(t.2) & (t.1=t_n)
            \end{cases}
          \]
          and define $\gamma^{n}_+(\A{R})\triangleq\{r|\alpha^{n}_+(r)\in\A{R}\}$ as in the previous section.
    \item Define $\A\EE_n$ and $\A\LL_n$ as:
          \begin{align*}
            \A\LL_0\triangleq\A{\sembracket{e_0}}(\alpha_0(s))    &  & \A\EE_0\triangleq\A\LL_0(e_0,\alpha_0(s))                                       \\
            \A\EE_{n+1}\triangleq\A{\mathsf{F}}\:\A\EE_n\:e_{n+1} &  & \A\LL_{n+1}\triangleq\A\LL\:\A\EE_n\:e_{n+1}\cup[\alpha_0(s)\mapsto\A\EE_{n+1}]
          \end{align*}
  \end{itemize}

  Then:
  \[
    \sembracket{l_n}(s)\sqsubseteq\bigcup_{i=0}^{n}{\gamma^i_+(\A\LL_i)}
  \]
\end{thm}

What the above theorem means is that there exists a concrete $\tick$ function that can be covered separately by analyzing each component based only on the approximation of the exported context.
The fact that the analysis $\A\LL_n$ can be computed without actually computing the final times $t_n$ is why this analysis can be called separate.
\bibliographystyle{ACM-Reference-Format}
\bibliography{citations}
\end{document}
%%% Local Variables: 
%%% coding: utf-8
%%% mode: latex
%%% TeX-engine: xetex
%%% End:
